# llm-with-chat-interface

- Chat Interface using React and FastAPI
- Using LangChain with locally ran model(mistral:latest) with Ollama to generate response